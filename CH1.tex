
%----------------------------------------------------------------------------------------
%	THESIS CONTENT - CHAPTERS
%----------------------------------------------------------------------------------------

\mainmatter % Begin numeric (1,2,3...) page numbering
\pagenumbering{arabic}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}\label{intro}

The idea originated from Dr. T. Bosse from the Chair for Advanced Computing at the Friedrich Schiller University in Jena. When proposing the idea for a master's thesis with the topic of "Music similarity measurement using genre-specific features" by using different guitar play styles in modern-day metal music, he jokingly said that he would also like to know how metal music compares to construction building noise. The idea is actually not so groundless, considering that most people would agree on the fact that metal music is often described as noise by people not used to listening to genres like death and black metal.
While refining the original idea of the theme for this master's thesis and during the first tests, it became apparent, that while there is a lot of research in the area of music similarity for single aspects of music like melody, timbre, or rhythm and even for a few fixed combinations thereof, there was no attempt made yet, to build a parameterizable system combining various of these features in a Big Data environment.\\
With music streaming services like Spotify, Amazon Music, Deezer or Tidal and music sharing websites like SoundCloud, access to millions of songs is given. To explore this humongous amount of data, the need for music recommender systems rises. SoundCloud Go+, the streaming service of SoundCloud alone gives access to more than 150 million songs~\cite{soundcloudgo}. 
Obviously, the streaming platforms are aware of these challenges. When using services like "[...] Spotify Radio, iTunes Radio, Google Play Access All Areas and Xbox Music. Recommendations are typically made using (undisclosed) content-based retrieval techniques, collaborative filtering data or a combination thereof."~\cite[p. 9]{knees1}\\
But music similarity is not well defined. This is one of the first problems while dealing with this topic. It is a rather subjective value that can differ from listener to listener. Two tracks could be considered as "similar" when they are equal in tempo, loudness, melody, instrumentation, key, rhythm mood, lyrics, or a combination of more than a few of these features.\\
\ \\
\section{Objectives}

The target of this thesis is to propose a transparent music similarity recommendation system based on various weighted aspects of the music instead of a fixed combination. Applying different weights to different features allows similarity retrieval methods to search for different kinds of similarities, empowering the user to decide which aspects are most important to the user and returning song recommendations based on the user's preferences. E.g., weighting the tempo and beat of a song more than melodic similarity allows the creation of playlists for workout and sport, while melodic/ timbre, etc. similarities allow searching for similar songs from musical subgenres.\\ 
The usage of a Big Data framework such as Spark allows the creation of a parameterized similarity definition. Various aspects of the music could easily be merged and taken into consideration when calculating the musical distance between two different pieces. This offers a more diverse music recommendation system than already existing ones.\\ 
To do this, a lot of different features are required and have to be extracted from the audio data first. Content (e.g., audio features) and context (e.g., listener behavior) data can then be fed into a Big Data framework to speed up operations. For this thesis, however, the focus lies on content-based data only.\\ 
Context-based collaborative-filtering techniques, which take the listening behavior of other users into consideration, in combination with Big Data frameworks are already well researched. But this thesis is meant to propose a user-centered recommendation engine, relying on musical properties of the songs only. By solely relying on the musical features of the songs, no biasing due to the popularity of artists is to be expected.\\

\section{Outline}

\noindent The thesis is structured into four main issues, pictured in Figure~\ref{structh}. These different problems are resolved throughout the chapters of the thesis.

\begin{figure}[htbp]
	\centering
	%\smartdiagramset{set color list={blue!40!white, blue!40!white,blue!40!white, blue!40!white, blue!40!white}}
	\tikzset{priority arrow/.append style={rotate=180,anchor=0,xshift=30,}}
	\smartdiagram[priority descriptive diagram]{Evaluation of the Results, Similarity Estimation, Feature Extraction, Data Aggregation}
	\captionof{figure}{Structure of the thesis}
	\label{structh}
\end{figure}

\noindent First of all, a lot of music data is required. In Chapter~\ref{audiofeat}, different scientific datasets and sources for audio files are evaluated. It also explains the basics of music information retrieval (MIR) and gives a short overview of different similarity measurements based on different audio features and aspects of the music. In the last section of this chapter an introduction to Big Data frameworks is given and the choice of Spark as the Big Data processing framework is explained.\\
In Chapter~\ref{simanal}, multiple algorithms and approaches for the computation of similarity between timbral, melodic and rhythmic features are evaluated and selected.\\
Chapter~\ref{implementationdet} explains the implementation of the feature extraction process in parallel on a cluster and the implementation of the recommender system with Spark.\\
In Chapter~\ref{bds2} the resulting song recommendations are proposed and evaluated, and lastly Chapter~\ref{summarych} summarizes all results and provides an outlook for possible enhancements. 


\chapter{Music Information Retrieval and Big Data}\label{audiofeat}

The field of music information retrieval is a large research area combining studies in computer science like signal processing and machine learning with psychology and academic music study. To get started, a brief overview is given in the next section providing the most important information about publicly available datasets, MIR toolkits, and different approaches to music similarity using various audio features. An overview over Big Data frameworks is included as well. More in-depth information about selected metrics is given in Chapter~\ref{simanal}. 

\section{Terminology}

To clarify the usage of a few terms throughout this thesis (especially later in Section~\ref{bds1}), the following list provides an overview of the terms used.

\begin{itemize}
	\setlength\itemsep{-0.5em}
	\item song request
	\item distance
	\item similarities
\end{itemize}

\noindent The term "song request" describes the song title passed to the recommendation engine to estimate the similarities.\\
\noindent The terms "similarities" and "distances" are used synonymously in this thesis because all the similarity estimations are based on distances between feature vectors of different feature types ($x$ and $y$), following the equation
\begin{equation} \label{eq:distsim}
%p (v \vert \lambda)=\sum_{i=1}^{M}w_iN(v \vert \mu_i,\sum_i) \label{prob}
\text{sim}(x, y) = \frac{1}{d(x, y)}.
\end{equation}
The smaller the distance $d(x, y)$ between the audio features of two songs $x$ and $y$ is, the greater the similarity $\text{sim}(x, y)$ between these songs gets.\\

\section{Audio Features}\label{featsec2}

This section provides a short overview of commonly used audio features in MIR, including:

\begin{itemize}
	\setlength\itemsep{-0.5em}
	\item Discrete Fourier Transform
	\item Mel Frequency Cepstral Coefficients
	\item Chroma features
	\item Pitch curve
	\item Onsets
	\item Beats
\end{itemize}
These audio features are the starting point for the later following calculation of the distances between songs.

\subsection{Fourier Transformation}\label{featsec}

Most of the algorithms for audio data analysis start with switching from the time domain to the frequency domain by performing a discrete Fourier transform (DFT) as described in the equation
\begin{equation} \label{eq:fft}
%p (v \vert \lambda)=\sum_{i=1}^{M}w_iN(v \vert \mu_i,\sum_i) \label{prob}
X_l = \sum_{n=0}^{N-1}{x_n \cdot e^ { - \frac{ 2 \cdot \pi \cdot i}{N}\cdot l\cdot n}}, \qquad l = 0,1,..., N-1
\end{equation}
and then computing the power spectrum
\begin{equation} \label{eq:absfft}
%p (v \vert \lambda)=\sum_{i=1}^{M}w_iN(v \vert \mu_i,\sum_i) \label{prob}
|X_l| = \sqrt{\operatorname{Re}(X_l)^2 + \operatorname{Im}(X_l)^2}, \qquad l = 0,1,..., N-1.
\end{equation}
The value $N$ resembles the frame/window size, $x_n$ is the $n^\text{th}$ input amplitude in the frame ranging from 0 to $N-1$, and $l$ is an integer also ranging from 0 to $N-1$ (as many frequency values are computed per frame as discrete-time values are in the window).\\
\noindent Sampling a song with length $t$ in seconds by a sample rate $f_s$ results in
\begin{equation} \label{eq:points}
%p (v \vert \lambda)=\sum_{i=1}^{M}w_iN(v \vert \mu_i,\sum_i) \label{prob}
K = f_s \cdot t
\end{equation}
data points $x$ in an audio file. Considering a sample rate $f_s = 44,1kHz$ (usual CD sample-rate) and the length of a song of about $t = 180s$, the time domain contains $K = 7938000$ data points usually with 16-bit resolution for mono-channel audio, following Equation~\eqref{eq:points}.\\
Calculating a DFT with a window size of $N = 1024$ samples and a hop size of 512 samples, the full resulting spectrogram would contain $N_{fv} = 11627$ frames with 1024 amplitude values per frame for a 3 minute example song sampled with $44.1 \text{kHz}$, according to~\cite[p. 56]{knees1}: %The hop size determines the number of values $x$ in the time domain skipped between each computation of the DFT.
\begin{equation} \label{eq:hop}
%p (v \vert \lambda)=\sum_{i=1}^{M}w_iN(v \vert \mu_i,\sum_i) \label{prob}
N_{fv} = 1.5 \cdot (\frac{44100 \ \text{samples/s}}{1024 \ \text{samples/frame}}) \cdot t
\end{equation}
\noindent The hop size determines how many discrete-time values are skipped between the computation of each DFT frame. In the example with a hop size of 512 and a window size of 1024 the various frames overlap by 50\%, resulting in the factor $1.5$ in Equation~\eqref{eq:hop}.\\
%The following plots were created with the librosa library~\cite{librosa1}.
\noindent As an example, figure~\ref{laylaspec} shows the resulting spectrogram (spectrum of frequencies over time) of the first bars of the song Layla by Eric Clapton recorded on an electric guitar.\\ 
\begin{figure}[htbp]
	\centering
	\begin{subfigure}{0.5\textwidth}
		\centering
		\framebox{\parbox{2.96in}{      
		\includegraphics[scale=0.25]{Images/Layla/laylacfft.png}}}
		\captionof{figure}{Spectrogram}
		\label{laylaspec}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\framebox{\parbox{2.96in}{      
		\includegraphics[scale=0.25]{Images/Layla/laylacfftlog.png}}}
		\captionof{figure}{Log-scaled spectrogram}
		\label{laylaspecfft}
	\end{subfigure}
	\caption{Example spectrograms linear (a) and log-scaled (b)}
	\label{fig:test}
\end{figure}
\FloatBarrier
\noindent Since the human ear perceives sound in a non-linear fashion, a logarithmic (see Figure~\ref{laylaspecfft}) or mel scale is more suitable to represent different pitches. For example, the note A4 is perceived at a frequency of 440Hz, the A note of next octave (A5) is at 880Hz and the next one is at 1600Hz and so on. The mel scale was introduced to resemble the non-linear human perception of frequency~\cite[pp. 53f]{knees1}. The conversion between a frequency $f$ in $\text{Hz}$ and $m$ in $\text{mel}$ is given by
\begin{equation} \label{eq:mel}
%p (v \vert \lambda)=\sum_{i=1}^{M}w_iN(v \vert \mu_i,\sum_i) \label{prob}
m = 1127 \cdot \text{ln}(1 + \frac{f}{700}).
\end{equation}
\noindent The high dimensionality of the spectrogram is a problem for machine learning applications and music similarity tasks, as computation based on vectors with such a high dimensionality on larger datasets would require excessive computational power, e.g., for real-time applications.
To further reduce the dimensionality of the feature vector resulting from the DFT, a possible approach in MIR would be to calculate the so-called Mel Frequency Cepstral Coefficients (MFCCs)~\cite[pp. 55ff]{knees1}.

\subsection{Mel Frequency Cepstral Coefficients}\label{mfccsim}

Of all features presented in this chapter, the MFCC is the hardest one to grasp because of its abstract nature and hardly visible relatedness to musical aspects of audio files like pitch or rhythm. This section gives a brief overview of the computation of the MFCC as stated in~\cite[pp. 55ff]{knees1}.
Figure~\ref{sweep} shows the magnitude spectrum of a logarithmic frequency sweep signal as an example for better understanding.
\begin{figure}[htbp]
	\centering
	\framebox{\parbox{1\textwidth}{ 
	\begin{subfigure}{.495\textwidth}
		\centering 
		\includegraphics[scale=0.25]{Images/MFCC/sweep.png}
		\caption{Spectrogram with linear frequency axis}
		\label{sweeplog}
	\end{subfigure}
	\begin{subfigure}{.495\textwidth}
		\centering
		\includegraphics[scale=0.25]{Images/MFCC/sweeplog.png}
		\caption{Spectrogram with logarithmic frequency axis}
		\label{sweeplin}
	\end{subfigure}
	}}
	\caption{Example spectrograms of a logarithmic frequency sweep}	
	\label{sweep}
\end{figure}
\FloatBarrier
\noindent First of all the magnitude spectrum is transformed to the mel scale following Equation~\eqref{eq:mel} by assigning each frequency value to a mel band.
Doing this, dimensionality reduction can be achieved by assigning multiple frequency values to one of typically 12 to 40 mel bands. The resulting vectors are then fed into a discrete cosine transformation (DCT) resulting in the MFCCs for each frame:
\begin{equation} \label{eq:dct}
%p (v \vert \lambda)=\sum_{i=1}^{M}w_iN(v \vert \mu_i,\sum_i) \label{prob}
X_k = \sum_{q=0}^{Q-1}{x_q \cos\left[{\frac{\pi}{Q}(q + \frac{1}{2})k}\right]}, \qquad k=0,1,...,Q-1
\end{equation}
where $Q$ denotes the amount of mel bands.
\begin{figure}[htbp]
	\centering
	\framebox{\parbox{1\textwidth}{ 
	\begin{subfigure}{.495\textwidth}
		\centering 
		\includegraphics[scale=0.25]{Images/MFCC/mfccscaled.png}
		\caption{MFCC high resolution}
		\label{mfcc}
	\end{subfigure}
	\begin{subfigure}{.495\textwidth}
		\centering
		\includegraphics[scale=0.25]{Images/MFCC/mfccnorm.png}
		\caption{MFCC 13 bands scaled}
		\label{mfccs}
	\end{subfigure}
	}}
	\caption{MFCCs of a logarithmic frequency sweep}	
	\label{fig:mfcc}
\end{figure}
\FloatBarrier
\noindent Figure~\ref{mfcc} shows the resulting MFCCs with a high resolution of 1024 mel bands. This is not what would be done in a usual application, because this is nearly as high-dimensional as the original spectrogram. In comparison, Figure~\ref{mfccs} shows the MFCC reduced to 13 mel bands.
To better visualize the MFCCs, all values are typically scaled to have a standard deviation of 1 and a mean value of 0 per band in the plots. 
\noindent To describe a tone, three moments can be used according to~\cite[pp. 15f]{musicdata}: %tonal intensity perceived as loudness, the tonal quality perceived as the pitch and the timbre or tonal color as the third moment. 
\begin{itemize}
	\setlength\itemsep{-0.5em}
	\item tonal intensity perceived as loudness
	\item tonal quality perceived as the pitch 
	\item timbre or tonal color
\end{itemize}

\noindent MFCCs were found to be suited to represent the timbral attributes of music~\cite[pp. 55 ff]{knees1}. Looking at an example melody line played on an electric distorted guitar and a piano, distinct differences can be seen in Figure \ref{fig:timbre}. 
\begin{figure}[htbp]
	\centering
	\framebox{\parbox{1\textwidth}{ 
	\begin{subfigure}{.495\textwidth}
		\centering 
		\includegraphics[scale=0.25]{Images/MFCC/timbre_eguitar.png}
		\caption{Guitar}
		\label{timbrep}
	\end{subfigure}
	\begin{subfigure}{.495\textwidth}
		\centering
		\includegraphics[scale=0.25]{Images/MFCC/timbre_piano.png}
		\caption{Piano}
		\label{timbreg}
	\end{subfigure}
	}}
	\caption{Spectrogram of a guitar (a) and piano (b) sample}	
	\label{fig:timbre}
\end{figure}
\FloatBarrier
\noindent Due to the physical properties of a string, every note played consists of the main frequency (the actually played note) and harmonic overtones because of the way a string, e.g. in a piano, vibrates and the wooden body resonates. 
\begin{figure}[htbp]
	\centering
	\framebox{\parbox{1\textwidth}{ 
	\begin{subfigure}{.495\textwidth}
		\centering 
		\includegraphics[scale=0.35]{Images/MFCC/mfcc_eguitar.png}
		\caption{Guitar}
		\label{mfccg}
	\end{subfigure}
	\begin{subfigure}{.495\textwidth}
		\centering
		\includegraphics[scale=0.35]{Images/MFCC/mfcc_piano.png}
		\caption{Piano}
		\label{mfccp}
	\end{subfigure}
	}}
	\caption{MFCCs of a guitar (a) and piano (b) sample}	
	\label{fig:timbrmfcce}
\end{figure}
\FloatBarrier
\noindent Typically the harmonics of a piano consist of the main key, the same key a few octaves higher and major thirds and fifths of the octave. Depending on the instrument, these harmonics decay faster or slower or do not appear at all. An electrically amplified guitar amplifies these overtones as well, which is visible in Figure~\ref{timbreg}.
\noindent These differences in timbre are also visible when looking at the MFCCs in Figure~\ref{fig:timbrmfcce}. This time the MFCC plots are pictured without the previously mentioned scaling. Additionally, the mean value and standard deviation of the MFCCs indexed from 4 to 13 are pictured in Figure~\ref{fig:timbrstatmfcce}. 
This calculation of statistical summaries of the MFCC features reduces the dimensionality of the MFCC features and is later explained in more detail in Section~\ref{musly}. 
Although both times the exact same melody is played in the same tempo, the MFCC features vary due to the different timbral properties of the instruments. 
\begin{figure}[htbp]
	\centering
	\framebox{\parbox{1\textwidth}{ 
	\begin{subfigure}{.495\textwidth}
		\centering 
		\includegraphics[scale=0.45]{Images/MFCC/stat_eguitar.png}
		\caption{Guitar}
		\label{mfccsg}
	\end{subfigure}
	\begin{subfigure}{.495\textwidth}
		\centering
		\includegraphics[scale=0.45]{Images/MFCC/stat_piano.png}
		\caption{Piano}
		\label{mfccsp}
	\end{subfigure}
	}}
	\caption{MFCCs mean and standard deviation of a guitar (a) and piano (b) sample}	
	\label{fig:timbrstatmfcce}
\end{figure}
\FloatBarrier

\subsection{Other Audio Features}\label{otheraudiofeat}

As another, better comprehensible, and higher-level set of features, the chromagram represents the melodic and harmonic properties of a song. The chroma plot shows the distribution of the different pitches mapped to the various semi-tones in one octave (see Figure~\ref{laylachroma}). 
\begin{figure}[htbp]
	\centering
	\framebox{\parbox{1\textwidth}{ 
			\begin{subfigure}{.495\textwidth}
				\centering 
				\includegraphics[scale=0.28]{Images/Layla/laylacmfcc.png}
				\caption{MFCC}
				\label{laylacmfcc}
			\end{subfigure}%
			\begin{subfigure}{.495\textwidth}
				\centering    
				\includegraphics[scale=0.28]{Images/Layla/laylachroma.png}
				\caption{Chroma Features}
				\label{laylachroma}
			\end{subfigure}%
	}}
	\caption{Melodic and timbral features of the song Layla by Eric Clapton}
	\label{fig:feat1}
\end{figure}
\FloatBarrier
\noindent The mapping can be done with the help of binning strategies on the spectral representation or with special non-uniform filter banks \cite[p. 153]{musicdata}. The chroma values of each time frame are then normalized to one by the strongest dimension. So if all values are close to one, it is most likely that there will be only noise or silence at that frame in the recording, as depicted in the first few frames (0 to around 0.75 seconds) in Figure~\ref{laylachroma}. The chromagram has one significant downside because it is reduced to one octave and thus can not represent the melody of a song to its full extent. The chromagram and the extraction of melody information from chroma features is further evaluated in Section~\ref{melsimc}.\\
\noindent Figure~\ref{laylapitch} shows the pitch curve of the recording. None but the most dominant frequencies are shown. Pitches below a certain threshold are filtered out. In contrast to the chromagram the pitch curve provides information over the whole spectrum and is not limited to one octave. These pitch curves can be used to estimate and transcribe musical notes from audio data as presented in Section~\ref{midiest}.\\
\noindent The low-level rhythmic features of a song include the estimation of the overall tempo, beats, and onset events. 
The plot in Figure~\ref{laylacbeat} depicts the onsets (blue) and estimated beats (red dotted lines) in the first few seconds from the guitar recording of the song Layla by Eric Clapton. The onsets resemble, e.g., detected note events and note changes. The onset detection is described in \cite[pp. 412 ff]{musicdata} and most of the toolkits presented in the next section include methods for onset detection. %The plots in this section were created using the librosa toolkit. 
%\FloatBarrier
\begin{figure}[htbp]
	\centering
	\framebox{\parbox{1\textwidth}{ 			
			\begin{subfigure}{.495\textwidth}
				\centering
				\includegraphics[scale=0.28]{Images/Layla/laylapitch.png}
				\caption{Pitch}
				\label{laylapitch}
			\end{subfigure}%
			\begin{subfigure}{.495\textwidth}
				\centering     
				\includegraphics[scale=0.26]{Images/Layla/laylabeat.png}
				\caption{Rhythm / beat}
				\label{laylacbeat}
			\end{subfigure}%
	}}
	\caption{Rhythm features of the song Layla by Eric Clapton}
	\label{fig:feat2}
\end{figure}
\FloatBarrier
%Higher-level rhythmic features like rhythm patterns and rhythm histograms will be later explained in \ref{rhythmsimc}. 

\section{MIR Toolkits}\label{mirtoolkit}

This section provides a short overview of available toolkits for MIR, note extraction, and similarity estimation between songs. Some of the toolkits are used in Chapter~\ref{implementationdet} for the extraction and pre-processing of the audio features.

\subsection{Low-Level Audio Feature Extraction}
To extract audio features like the ones presented in Section~\ref{featsec2} (MFCCs, chromagram, beats, onsets) a wide variety of toolkits is publicly available and a few are presented in~\cite{audiofeattoolb}.
The YAAFE toolkit~\cite{yaafe1} is capable to extract a lot of different audio features like energy, MFCC, or loudness directly into the Hierarchical Data Format (*.h5) making it ideal for Big Data frameworks to use. It can be used with C++, Python~\cite{pylang}, or MATLAB~\cite{matl1}.\\
The Essentia toolkit~\cite{essentia1} is fairly similar to YAAFE, extending it by the calculation of rhythm descriptors, bpm, etc. It can also be used with C++ and Python.\\
The Librosa Toolkit provides similar functionality~\cite{labrosa1} as Essentia. It is user-friendly, well-documented, and can be used from within a Jupyter-Notebook~\cite{jupyter}, allowing rapid prototyping and testing of different algorithms. Most of the plots in this chapter were created using librosa. Code snippets for the extraction of low-level features with Essentia and librosa are given in Section~\ref{simmet} as well as a performance analysis of both.\\

\subsection{Music Similarity}
The MIR Toolkit~\cite{mirtoolbox1} is a toolbox for MATLAB. A port to GNU Octave~\cite{octave1} is also available~\cite{mirtoolbox2}. The Code Snippet~\ref{lst:MIRmat} is all it takes to compute a similarity matrix based on MFCC features, but the calculation is rather slow.

\lstset{language=Matlab}          % Set your language (you can change the language for each code-block optionally)
%\newpage
\FloatBarrier

\begin{lstlisting}[frame=single,label={lst:MIRmat},
caption={MATLAB code for estimating similarities based on MFCCs},captionpos=b]  % Start your code-block
numfiles = 326;
mydata = cell(1, numfiles);
for k = 1:numfiles
	myfilename = sprintf('%d.wav', k);
	mydata{k} = mirmfcc(myfilename);
	close all force
endfor
simmat = zeros(numfiles,numfiles);
for k = 1:numfiles
	for l = 1:numfiles
		simmat(k, l) = mirgetdata( ...
		mirdist(mydata{k}, ...
		mydata{l}));
	endfor
endfor
\end{lstlisting}
\FloatBarrier
\noindent An other easy way to test state-of-the-art music similarity algorithms is to use the open-source toolkit Musly~\cite{musly1}. It is based on statistical models of MFCC features and calculates the similarities between songs very quickly, supporting OpenMP acceleration. It automatically extracts the features it requires from the audio files. To compare the extracted features and calculate the distances, it implements the method introduced by Mandel-Ellis~\cite{mandelellis1} and a timbre based improved version of the Mandel-Ellis algorithm using a Jensen-Shannon-like divergence~\cite{musly2}. More details and a re-implementation of some of the features from this toolkit are presented in Section~\ref{musly}.\\

\subsection{Melody / Pitch Extraction}\label{midiest}
\begin{figure}[htbp]
	\centering
	\framebox{\parbox{1\textwidth}{ 
	\begin{subfigure}{0.5\textwidth}
		\centering 
		\includegraphics[scale=0.4]{Images/felise.png}
		\caption{Für Elise~\cite{fe1}}
		\label{fe}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering      
		\includegraphics[scale=0.6]{Images/rachm.png}
		\caption{Rachmaninoff prelude in C\# minor~\cite{rm1}}
		\label{rm}
	\end{subfigure}%
	}}
	\caption{Original scores, Rachmaninoff (a) and Beethoven (b)}
	\label{fig:sheets}
\end{figure}
\FloatBarrier
\noindent To test the various pitch extraction toolkits, one piece by Rachmaninoff and one composed by Beethoven was used. Figure~\ref{fe} shows the first five bars of Beethoven's Bagatelle in A Minor ("Für Elise"). Two bars of Rachmaninoff's Prelude in C\# minor can be found in Figure~\ref{rm}. 
\begin{figure}[htbp]
	\centering
	\framebox{\parbox{1\textwidth}{ 
	\begin{subfigure}{0.5\textwidth}
		\centering      
		\includegraphics[scale=0.4]{Images/feliseaubio.png}
		\caption{Für Elise Aubio pitch}
		\label{fea}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering 
		\includegraphics[scale=0.4]{Images/rachaubio.png}
		\caption{Rachmaninoff prelude Aubio pitch}
		\label{raa}
	\end{subfigure}
	}}
	\caption{Pitch extraction with Aubio}
	\label{fig:aubio}
\end{figure}
\FloatBarrier
\noindent The first toolkit tested is called "aubio"~\cite{aubio1}. The result can be seen in Figure~\ref{fea} and Figure~\ref{raa}. The upper subplot shows the waveform of the first few seconds of each piece. The second subplot figures the estimated pitch with green dots. If the pitch is zero, then no pitch can be estimated, most likely because the associated frame contains silence. The blue dots resemble the estimated pitches where the confidence (shown as the blue graphs in the third subplot) is above a certain threshold (orange lines in the third subplots).
\noindent The other melody extraction tool is called "Melodia"~\cite{melodia1}, which is available as a VAMP plugin and can be used together with the "Sonic Visualiser"~\cite{sonviz1}.\\
The results are shown in Figure~\ref{fem} and~\ref{ram}.
The purple line resembles the estimated pitch; however, there are unwanted jumps between different octaves of the harmonics. 

\begin{figure}[htbp]
	\centering
	\framebox{\parbox{1\textwidth}{ 
	\begin{subfigure}{0.5\textwidth}
		\centering      
		\includegraphics[scale=0.13]{Images/femelodia.png}
		\caption{Für Elise Melodia pitch}
		\label{fem}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering 
		\includegraphics[scale=0.13]{Images/rmmelodia.png}
		\caption{Rachmaninoff Melodia pitch}
		\label{ram}
	\end{subfigure}
	}}
	\caption{Pitch extraction with Melodia}
	\label{fig:melodia}
\end{figure}

\noindent Music related information, e.g, about note length, tempo, etc., would typically be stored digitally into standard MIDI (Musical Instrument Digital Interface) files~\cite[p. 180]{musicdata}. Unfortunately, the conversion from the extracted pitches to MIDI notes does not work flawlessly. It is apparent in Figure~\ref{fig:transc} that the transcription does not work accurately enough, even for a classical music piece with only one instrument. Figure~\ref{femm} shows the output of a Python script using the Melodia VAMP plugin to calculate a MIDI file containing the main melody line, and Figure~\ref{feam} shows the transcribed MIDI notes from Aubio. The detected melody lines are jumping between different octaves, and finding the right threshold for the separation between silence and detected notes turns out to be problematic as well.

\begin{figure}[htbp]
	\centering
	\framebox{\parbox{1\textwidth}{ 
	\begin{subfigure}{0.5\textwidth}
		\centering      
		\includegraphics[scale=0.35]{Images/femelodiamidi.png}
		\caption{Für Elise Melodia MIDI}
		\label{femm}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering 
		\includegraphics[scale=0.33]{Images/feam.png}
		\caption{Für Elise Aubio MIDI}
		\label{feam}
	\end{subfigure}
	}}
	\caption{MIDI transcription Für Elise}
	\label{fig:transc}
\end{figure}
\FloatBarrier 

\section{Music Similarity Measurements}

This section gives an introduction to the possibilities of the estimation of similarities based on the proposed audio features. Selected metrics and similarity measurements are selected and will be later evaluated in Chapter~\ref{simanal}.

\subsection{Timbre Based}

The proposed approach by Dominik Schnitzer~\cite{schnitzer1}, creator of the Musly toolkit introduced earlier, is to take MFCCs as low-level features and then compute statistical features like mean, standard deviation, and covariances of the different MFCCs to reduce dimensionality before computing similarities. Another example for the computation of approximate nearest neighbors was published in the paper titled "Large-scale music similarity search with spatial trees" by Brian McFee and Gert Lanckriet~\cite{msd4}.\\ 
A selection of different timbre based similarity measurements is evaluated later in Section~\ref{musly}.

\subsection{Pitch Based}

One proposed approach by Matija Marolt in 2006 is to take mid-level melodic representations of audio files like the chromagram instead of high-level features like sheet music or low-level features like Gaussian mixture models of MFCCs, to compute the similarity between songs~\cite{pitch1}. A more detailed analysis of this topic is given in Section~\ref{melsimc}.

\subsection{Note Based}

For comparing musical pieces by their symbolic representation (notes, tablatures, etc.), different text retrieval methods can be used. MIDI files as a digital representation of notes are a good starting point. For example Xia (et. al) uses a variation of the Levenshtein distance measurement to compute similarities between MIDI files~\cite{chroma4}. 
The problem with notation based algorithms is that there are not many datasets available containing audio and MIDI information. As shown in Section~\ref{midiest}, the automatic transcription of notes from raw audio does not work flawlessly. There is still ongoing research to automatically annotate musical notes with the help of neural networks (for example~\cite{crepe1}).
In Section~\ref{rhythmsimc} an attempt to extract note information as text features from chromagrams and calculating the similarity by using the Levenshtein distance is shown and evaluated.

\subsection{Rhythm Based}

Rhythm based music similarity algorithms use timing information of various events as a baseline. For example low-level features like the onset and beat data from the plot in Figure~\ref{laylacbeat} could be used as a starting point for rhythmic similarity retrieval. As an example, Foote (et. al) introduced a feature called the "beat spectrum" for the computation of rhythmic similarities~\cite{rhythm1}. Other more recent or advanced approaches make use of the rhythm histogram, beat histogram, and rhythm patterns later evaluated in Section~\ref{rhythmsimc}.

\subsection{Metadata Based / Collaborative Filtering}\label{collaborative}

Most of the research that combines the field of music information retrieval with Big Data frameworks relies on data based on the listening behavior of many users of, e.g., music streaming platforms. In 2012 the Million Song Dataset (MSD) Challenge was brought to the MIR community. Researchers were challenged to give a list of song recommendations based on a large set of user data, the Million Song Dataset (see Section~\ref{datasets} for more details on the dataset). As an example, if user X listens a lot to artist A and B and user Y listens mostly to artist A and C, then user X could probably like artist C as well. These kinds of collective listening behavior based recommendations are called "collaborative filtering" and are pretty common in large music streaming services, although not necessarily representing direct musical similarity.~\cite[pp. 192f]{knees1}\\
Recommendation systems based on collaborative filtering tend to propose commonly well-known artists rather than not so well-known ones, possibly biasing the resulting recommendation. On the other hand, these kinds of similarity algorithms can work very fast and efficient in a Big Data environment. The usage of annotations and metadata information like genre and artist based recommendations are common as well. The recommendation of songs based on lyrics and also hybrid recommendation systems that combine lyrics, metadata, and collaborative filtering are also possible.\\
However, all of these recommendation strategies are not directly based on musical features and are, therefore, not evaluated further throughout this thesis. But they are a possible addition for a hybrid recommendation engine for future research. 
An example using user-based collaborative filtering is the paper "Design and Implementation of Music Recommendation System Based on Hadoop"~\cite{metadat1}. 

\subsection{Genre Specific Features}

The impact of the choice of parameters for similarity measurements on different music subgenres was evaluated by Gulati (et al.) for Indian art music (Carnatic and Hindustani music)~\cite{mussim1}. They state: "We evaluate all possible combinations of the choices made at each step of the melodic similarity computation [...].  We consider 5 different sampling rates of the melody representation, 8 different normalization scenarios, 2 possibilities of uniform time-scaling and 7 variants of the distance measures.  In total, we evaluate 560 different variants"~\cite[p. 3]{mussim1}. This evaluation showed that the choice of features and parameters for music similarity measurement is a critical point. "Sampling rates do not have a significant impact for Hindustani music, but can significantly degrade the performance for Carnatic music."~\cite[p. 3]{mussim1}. So using different kind of feature sets and parameters for the recommendation of songs from different genres could be an option but would go beyond the frame of this thesis.\\
As another idea, e.g., in Rock, Pop and Metal music, the analysis of different guitar playing techniques would be beneficial. Guitar tablature extraction~\cite{guitext1} toolkits could be used to extract information, whether the guitar in a song is, for instance, mostly picked or strummed or if there are hammer-on, pull-off, side bending, or tapping techniques used. In classical music, the play style of the string section of an orchestra could be taken into consideration (staccato, pizzicato, etc.). These kinds of information could be used as a baseline for song recommendations. 
However, there is no MIR toolkit available for the estimation of play styles, so this idea would have to be evaluated in future research.

\subsection{Selection}

In this thesis, music similarity measurements based on three different types of features are evaluated. The first is based on MFCCs to represent timbral features of the songs and therefore offering a set of features to make recommendations that are similar in tone color and should be able to give recommendations inside the boundaries of different genres. The second is based on chroma features and note information to provide a measurement of melodic similarity. Utilizing these features targets the detection of cover versions. The third set of features is based on the rhythmic properties of a song. This should enable the recommendation of songs with the same tempo and rhythmic structure, and possibly also enable the recommendation of songs within the same genre.\\
The usage of MIDI files is not considered further due to the rather poor performance of automatic score extraction tools for songs with multiple instruments and melody lines, and the lack of datasets containing MIDI and audio files. Also, the melodic component of the songs is already represented by the chroma features, although that limits the representation to one or at most very few octaves.\\
Collaborative filtering is left out because it does not necessarily represent the musical features and properties but instead the personal taste of other people. Additionally, it is left out because no fitting dataset with the required information and matching music files was found (see Section~\ref{data}).\\
Lastly, genre-specific features are also not an option because this field is not very well-researched yet, and the development of algorithms and the extraction of features would go beyond the scope of this thesis.

\section{Data Aggregation}\label{data}

To evaluate the music similarity algorithms and metrics, a lot of music data is needed. This section provides an overview of publicly available sources for audio data. A selection from which the audio features are extracted is given in Section~\ref{tdset}.

\subsection{Datasets}\label{datasets}

\subsubsection{Free Music Archive}

The largest dataset is the Free Music Archive (FMA) consisting of 106733 different songs totaling an amount of nearly one terabyte of music data from a variety of different music genres~\cite{fma1} (see Figure~\ref{fmadist}). There is also a lot of metadata like genre tags available for most of the songs.

\subsubsection{Private Music Collection}

The private music collection used in this work consists mainly of metal music. The music was legally purchased; all rights belong to the respective owners. Therefore this dataset can not be published alongside this thesis. But the private music collection is fully cataloged, and the according PDF file is in the appendices. The distribution of different songs per genre for this dataset is listed in Figure~\ref{privmusdist}.\\
\noindent Additionally, a private recording dataset was used, consisting of ambient recordings and self-produced music. Most of these files are available on SoundCloud~\cite{bqpd1}.\\
Because music recommendations are always related to personal taste and the perception of the quality of the results may differ, the inclusion of the private music collection is necessary to enable a subjective evaluation of the results from the developed recommendation engine.

\subsubsection{1517-Artists and Musicnet}

Other sources of music are the Musicnet dataset~\cite{musicnet1} and the 1517-Artists dataset~\cite{1517artists1}. The Musicnet dataset includes 330 pieces of classical music with musical note values and positions as annotations and the 1517-Artists dataset contains 3180 songs of multiple genres (see Figure~\ref{1517dist}). 

\subsubsection{MedleyDB}

For a melody or pitch based similarity analysis, multitrack datasets could provide useful data, because the pitch estimation can be done instrument by instrument. There are, e.g., the MedleyDB~\cite{medleydb1} (see Figure~\ref{medleydbdist}) and MedleyDB2~\cite{medleydb2} datasets, as well as the Open Multitrack Dataset~\cite{openmult1} currently consisting of 593 multitracks in which the MedleyDB dataset is already included, leaving 481 other tracks for analysis.

\subsubsection{Covers80}\label{cov801}

For cover song detection analysis, the covers80 dataset is available~\cite{cover80} containing eighty original songs predominantly from the musical genres rock and pop and 84 cover versions. These cover versions tend to differ significantly from the original in musical style, rhythm, and timbre.
%The ability to detect cover songs or different versions/ recording of a musical piece could be a good measurement for the efficiency of music similarity algorithms. In the next chapter~\ref{simanal}, one test case is presented, showing, that for instance an MFCC based music similarity algorithm isn't able to detect different recordings of the same piano piece as most similar to each other. 

\subsubsection{Overview and Other Sources}

\begin{figure}[htbp]
	\centering
	\framebox{\parbox{1\textwidth}{ 
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[scale=0.35]{Images/fma_genre.JPG}
		\caption{FMA~\cite[p. 4]{fma1}}
		\label{fmadist}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[scale=0.45]{Images/privmus.jpg}
		\caption{Private music collection}
		\label{privmusdist}
	\end{subfigure}
	
	\begin{subfigure}{0.55\textwidth}
		\centering
		\includegraphics[scale=0.33]{Images/1517genre.png}
		\caption{1517-Artists}
		\label{1517dist}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[scale=0.31]{Images/MedleyDB1.png}
		\caption{MedleyDB~\cite[p. 2]{medleydb1}}
		\label{medleydbdist}
	\end{subfigure}
	}}
	\caption{Genre distribution of songs in various datasets}
	\label{fig:datasetdist}
\end{figure}
\FloatBarrier

\noindent The music sources and amounts of songs used for the task at hand are listed in Table~\ref{table_dsets}.

\begin{table}[h]
	\begin{center}
		\begin{tabular}{|c||c|c|}
			\hline
			dataset & \#songs & features\\
			\hline
			\hline
			FMA & 106.733 & -\\
			\hline
			private & 8.484 & -\\
			\hline
			1517-Artists & 3.180 & -\\
			\hline
			Maestro & 1.184 & MIDI (piano sheet music)\\
			\hline
			musicnet & 330  & note annotation\\
			\hline
			Open Multitrack Testbed & 593(481) & multitracks\\
			\hline
			covers80 & 164  & 80 originals + 84 covers\\
			\hline
			MedleyDB &  122  & multitracks\\
			\hline
			MedleyDB2 &  74  & multitracks\\
			\hline
		\end{tabular}
	\end{center}
	\caption{Number of songs in different music datasets}
	\label{table_dsets}
\end{table}
\FloatBarrier

\subsection{Alternatives}

\subsubsection{Spotify API}\label{spotipy}

Another way of getting music samples, audio features, and metadata could be by using the Spotify API~\cite{spotifyapi1}.
%A part of the available audio features comes from the Echo Nest\cite{echonest1}.
The downside of using the Spotify API is that no packed and ready to use test dataset containing the relevant features is available. Therefore, for scientific purposes, a test dataset would have to be created first. Using a small Python library named Spotipy~\cite{spotipy1}, the available information can be accessed very easily.\\
%For the purpose of this thesis, the option of creating an own dataset using the Spotify API and Spotipy was considered and ten very small test playlists of different genres were created using the Spotify Playlist Miner~\cite{spotmin1}. 
Appendix~\ref{spotcrawl} lists a small script, that is able to download all audio features and analysis data from selected songs in a playlist that contain a preview URL to a 30-second audio snippet. The audio features and analysis data is saved as a JSON file containing information about:
\begin{itemize}
	\setlength\itemsep{-0.5em}
	\item acousticness
	\item danceability
	\item instrumentalness
	\item liveness
	\item loudness
	\item speechiness
	\item valence
	\item predicted key
	\item tempo 
	\item pitch 
	\item tempo 
	\item timbre information 
	\item beats and bars 
\end{itemize}
\noindent In Figure~\ref{spp} the returned chroma features (using the script in Appendix \ref{spotcrawl}) of the piano piece "Für Elise" by Beethoven are shown and Figure~\ref{spp2} shows the beginning of the piece in more detail, including green dots that resemble estimated bar markings. The blue dots represent the note values of one octave. That means they can resemble a value between zero and eleven with 0 representing the key C and 11 representing a B. The Spotify API actually returns a chroma feature value for every single one of the semi-tones per segment, with one segment being a section of samples that are relatively uniform in timbre and harmony. But in the plots, only the most dominant key per segment is shown to visualize the main melody line.
\begin{figure}[htbp]
	\centering
	\framebox{\parbox{1\textwidth}{ 
	\begin{subfigure}{0.5\textwidth}
		\centering      
		\includegraphics[scale=0.22]{Images/spot1.png}
		\caption{Für Elise Spotify pitch}
		\label{spp}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering 
		\includegraphics[scale=0.28]{Images/spot2.png}
		\caption{Für Elise detail}
		\label{spp2}
	\end{subfigure}
	}}
	\caption{Extracted pitches, Spotify API (Spotipy)}
	\label{fig:spotify}
\end{figure}
\FloatBarrier
\ \\
Together with the 30-second audio sample from which more features like MFCCs could be extracted, Spotipy could provide all the information needed to build a large dataset for MIR. However, the terms and conditions explicitly prohibit crawling the Spotify service. As stated by the Spotify "Terms and Conditions of Use", section 9 (User guidelines):\\
"The following is not permitted for any reason whatsoever:\newline
[...]\\
12. “crawling” the Spotify Service or otherwise using any automated means (including bots, scrapers, and spiders) to view, access, or collect information from Spotify or the Spotify Service"~\cite{spottac1}\\
Therefore a larger dataset based on the Spotify API can not be created without the risk of legal infringements. %Upon request the Spotify API developer team did not respond and therefore in this thesis the Spotify API wont be used to create a test dataset.\\ 
One could argue that there was a difference between data mining and data crawling and for small datasets these restrictions may not apply.
%, when the purpose the recommendations would be the creation of Spotify playlists.\\ 
%In the sense of the Spotify Developer Terms of Service~\cite{spottac2} there may be no legal infringements by creating a non-commercial playlist generation tool. 
Spotify states that by creating an algorithmically generated playlist similar to the "Discover Weekly" playlists one may encounter legal problems if using such features commercially~\cite{spottac3}. However it does not prohibit the usage for non-commercial cases.\\ 
Upon an initial request, the Spotify API developer team did not respond and therefore in this thesis the Spotify API will not be used to create a test dataset. Without further reaching out to Spotify, using the Spotify API to create a test dataset is not an option. 

\subsubsection{Million Song Dataset}

Another outstanding and very large dataset is the Million Song Dataset (MSD)~\cite{msd1}. 
It contains a large set of metadata per track as well as a lot of supplementary datasets, like the tagtraum genre annotation (Figure~\ref{msddist})~\cite{msd5} and the Last.fm dataset~\cite{msd2}. In addition to that, the Echo Nest API dataset contains a lot of additional audio features like pitch, loudness, energy, and danceability to name just a few~\cite{msd3}. 
Another addition is the SecondHandSongs Dataset~\cite{msd6}, containing a list of cover songs in the Million Song Dataset.
\begin{figure}[thpb]
	\centering
	\framebox{\parbox{3in}{      
			\includegraphics[scale=0.5]{Images/MSD_Tagtraum.JPG}}}
	\caption{Million Song Dataset genre distribution~\cite[p. 6]{msd5}}
	\label{msddist}
\end{figure}
\FloatBarrier
\noindent Due to the fact that the Spotify API~\cite{spotifyapi1} also works with audio features from the Echo Nest~\cite{echonest1}, the MSD could be used in a Big Data environment to simulate the work with Spotify data, without the need of mining the actual data. The MSD was already used with Big Data frameworks for music similarity retrieval based on metadata and user information (see~\cite{msd4}). Although the MSD does not contain any audio files in the first place, 30-second samples could be gathered through simple scripts from 7digital.com when the dataset was made publicly available. Unfortunately, 7digital does not offer the download of the 30-second sample files any longer, which makes this dataset impractical for this thesis, because missing audio features like MFCCs can not be computed from the audio files itself. 
