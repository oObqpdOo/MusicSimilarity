%==============================================================

\chapter{Implementation}\label{implementationdet}

The implementation consists of two separate parts. The first one contains the feature extraction and preparation of the data from the audio files. The results are stored in the feature files. These features files then have to be processed with the big data framework Spark to compute the similarities between songs.\\ 
Both parts are implemented in Python and can be executed on computer clusters. The source code can be found on the CD in the appendices and it can be pulled from GitHub \cite{github-code}. Details for the usage of the python scripts are also documented there.

\section{Audio Feature Extraction}\label{simmet}

So far the required audio features as well as toolkits to extract those features from the audio data have been selected in Chapter \ref{audiofeat}.
In Section \ref{data}, different sources for audio files have been presented. Section \ref{musly}, \ref{melsimc}, and \ref{rhythmsimc} presented algorithms to pre-process the low-level features and use these to compute similarities. 
This section focuses on the selection of fitting datasets to extract features from and the performance of the feature extraction and pre-processing software implementation.  

\subsection{Test Datasets}

A lot of data is needed to test the algorithms in a big data environment, so the Free Music Archive with its over 106000 songs is a solid option for performance tests. It hast to be kept in mind, that the genre distribution in the FMA dataset is quite one sided. Most of the songs are tagged as experimental, electronic and rock. Also this dataset may not be representative for actual popular music, a lot of the songs are live recordings with poor audio quality. %possibly influencing the results.
The 1517-Artists dataset offers 19 different genres with songs relatively equally distributed. For an objective evaluation of the proposed algorithms, e.g., by genre recall, this dataset is ideal. For cover song detection, the covers80 dataset is included as well.
The last source used in this thesis is the private music collection. This collection is biased towards metal music, but due to the match with personal taste, it enables a subjective evaluation of the results from the implemented recommender system.\\ 
In conclusion that sums up to about 117000 songs for performance tests and about 12000 songs for a detailed evaluation of the algorithms in this thesis, from which at the end overall 114210 could be used (see Section \ref{totamsong} for the details on the dropout). As mentioned in \ref{datasets}, all albums from the private music collections are cataloged as well, and the associated document is in the appendices. 

\begin{table}[h]
	\label{used_dsets}
	\begin{center}
		\begin{tabular}{|c||c|}
			\hline
			FMA & 106.733 Songs\\
			\hline
			private & 8484 Songs\\
			\hline
			1517-Artists & 3180 Songs\\
			\hline
			covers80 & 164 Songs (80 originals + 84 covers)\\
			\hline
		\end{tabular}
	\end{center}
	\caption{appropriate music datasets}
\end{table}
\FloatBarrier

\subsection{Feature Extraction Performance}

After evaluating the different features in the last three chapters, this section only discusses the performance of the feature extraction process without going too much into the details of the code for the feature pre- and post-processing, like the note estimation from the chroma features and the calculation of statistic features from the MFCCs. These additional steps were already explained in-depth in the previous chapters and are therefore left out here. The full code is in the appendices. 

\subsubsection{Librosa}

For most of the plots in Chapter \ref{audiofeat}, the python toolkit librosa was used because of its ease of use and very good documentation. The following code example shows the necessary methods to extract the most important features like MFCC, chromagram and beats/ onsets.
\lstset{language=Python} 
\begin{pythonCode}[frame=single,label={lst:librosa},caption={librosa},captionpos=b]
path = ('music/guitar2.mp3')
x, fs = librosa.load(path)
mfcc = librosa.feature.mfcc(y=x, sr=fs, n_mfcc=12)
onset_env = librosa.onset.onset_strength(x, fs, aggregate=np.median)
tempo, beats = librosa.beat.beat_track(onset_envelope=onset_env,sr=fs)
times = librosa.frames_to_time(np.arange(len(onset_env)), sr=fs, hop_length= 512)
chroma = librosa.feature.chroma_stft(x, fs)
\end{pythonCode}	
But when extracting features from batches of audio files, the librosa library turned out to be very slow. For a tiny dataset of 100 songs, the extraction of just the mean, variance, and covariance of the MFCCs and the estimated notes from the chromagram took about 48 minutes. 
For larger datasets like the 1517-Artists dataset, the feature extraction process would have taken about 22 hours. 

\subsubsection{Essentia}

Moffat (et al.) compares different Audio feature extraction toolboxes and shows that Essentia is a much faster alternative to librosa due to the underlying C++ Code and provides even more features, but it is a bit less well documented and requires more effort in implementation at the same time \cite{audiofeattoolb}.\\ 
In the end, the code to extract the necessary features had to be rewritten for the usage of Essentia due to the slow performance of librosa. Essentia offers two different ways to handle audio files. The first one is to use the Essentia standard library. It provides similar methods to librosa and uses an imperative programming style. The audio file has to be read, sliced and pre-processed by hand. The second way is to use Essentia streaming. Basically, a network of connected algorithms is created, and they handle and schedule the "how and when" of the execution whenever a process is called. \\
The melodic and timbral features and the beat histograms are all computed with Essentia. Only the rhythm patterns and rhythm histograms are computed in a separate step, as stated below. 

\subsubsection{Essentia Standard}

In the final code for the audio feature extraction, the computation of the MFCCs and beat histogram is done with the Essentia standard library, because it offers a fast and easy way to implement the basic feature extraction tasks. 
\begin{pythonCode}[frame=single,label={lst:esss},caption={Essentia standard},captionpos=b]
audio = es.MonoLoader(filename=path, sampleRate=fs)()
hamming_window = es.Windowing(type='hamming')
spectrum = es.Spectrum()
mfcc = es.MFCC(numberCoefficients=13)
mfccs = numpy.array([mfcc(spectrum(hamming_window(frame)))[1] 
	for frame in es.FrameGenerator(audio, frameSize=2048, hopSize=1024)])
rhythm_extractor = es.RhythmExtractor2013(method="multifeature")
bpm, beats, beats_confidence, _, beats_intervals = rhythm_extractor(audio)
peak1_bpm, peak1_weight, peak1_spread, peak2_bpm, peak2_weight, peak2_spread, histogram =
	es.BpmHistogramDescriptors()(beats_intervals)
\end{pythonCode}

\subsubsection{Essentia Streaming}

The Essentia streaming library is used to calculate the chroma features in the final code. It eases up the filtering with the high- and a lowpass filter. The audio signal is passed through various stages of processing and ultimately results in the chroma features of the high-pass filtered audio signal. 
%\newpage
\begin{pythonCode}[frame=single,label={lst:essst},caption={Essentia streaming},captionpos=b]
loader = ess.MonoLoader(filename=path, sampleRate=44100)
HP = ess.HighPass(cutoffFrequency=128)
LP = ess.LowPass(cutoffFrequency=4096)
framecutter = ess.FrameCutter(frameSize=frameSize, hopSize=hopSize, 
	silentFrames='noise')
windowing = ess.Windowing(type='blackmanharris62')
spectrum = ess.Spectrum()
spectralpeaks = ess.SpectralPeaks(orderBy='magnitude', magnitudeThreshold=0.00001, 
	minFrequency=20, maxFrequency=3500, maxPeaks=60)
hpcp = ess.HPCP()
hpcp_key = ess.HPCP(size=36, referenceFrequency=440, bandPreset=False, minFrequency=20,
	maxFrequency=3500, weightType='cosine', nonLinear=False, windowSize=1.)
key = ess.Key(profileType='edma', numHarmonics=4, pcpSize=36, slope=0.6, 
	usePolyphony=True, useThreeChords=True)
pool = essentia.Pool()
loader.audio >> HP.signal
HP.signal >> LP.signal
LP.signal >> framecutter.signal    
framecutter.frame >> windowing.frame >> spectrum.frame
spectrum.spectrum >> spectralpeaks.spectrum
spectralpeaks.magnitudes >> hpcp.magnitudes
spectralpeaks.frequencies >> hpcp.frequencies
spectralpeaks.magnitudes >> hpcp_key.magnitudes
spectralpeaks.frequencies >> hpcp_key.frequencies
hpcp_key.hpcp >> key.pcp
hpcp.hpcp >> (pool, 'tonal.hpcp')
essentia.run(loader)
chroma = pool['tonal.hpcp'].T
\end{pythonCode}	

\subsubsection{Essentia Performance}

The calculation with the Essentia streaming and standard library for 100 songs took less than half of the time librosa needed. This is a significant improvement, however the Essentia library uses only one CPU core so that performance was further improved by using the parallel python library.

\subsubsection{Parallel Python}

Parallel python is a Python module that enables the execution of python code in parallel. On a single PC, multiple CPU cores get a part of the filelist of all songs and can compute the features fully parallel.
%\newpage
\begin{pythonCode}[frame=single,label={lst:pp},caption={parallel python},captionpos=b]
job_server = pp.Server()
job_server.set_ncpus(ncpus)
jobs = [ ]
for index in xrange(startjob, parts):
	starti = start+index*step
	endi = min(start+(index+1)*step, end)
	jobs.append(job_server.submit(parallel_python_process, (index, 
		filelist[starti:endi],1,1,1,1,1)))
	gc.collect()
times = sum([job() for job in jobs])
job_server.print_stats()
\end{pythonCode}
The computation time takes about 15.4 seconds per song and processor core. Using 4 CPU cores for 100 songs, the overall processing time could be reduced to about 385 seconds. 
\begin{equation} \label{eq:parallelp}
time = \frac{\#songs}{\#CPUs} \cdot 15.4s
\end{equation}
Parallel python also opens up the possibility to use a cluster instead of a single-node PC.\\
For convenience, every processor gets a batch of files instead of single songs. For every batch, different output files for the various features are created. The batch size determines the overall size of these feature-files. For example, for the 1517-Artists dataset a batch size of 400 songs was chosen, so overall four CPUs had to process two batches, resulting in eight different output files with the chroma feature files being the largest with about 25MB per file.\\
One problem that appeared when using parallel python was that the main memory usage increased over time. The explicit usage of the garbage collector and the deletion of unwanted objects also couldn't solve that problem. So after processing a few hundred songs, the processes eventually ran out of memory and had to be restarted. By replacing parallel python with mpi4py, this problem could be solved later (see Section \ref{mpi4py}). 

\subsubsection{Rp\_extractor}

For the extraction of the rhythm patterns and rhythm histogram features as described in Section \ref{rhythmsimc}, the rp\_extractor tool provided by the TU Wien was used. Although running in parallel on all CPU cores on a single node, the extraction of the features from 100 songs takes about 442 seconds.

\subsubsection{Performance on a Single PC}

The extraction of the rhythm patterns and the rhythm histogram is performed by the rp\_extractor tool. The feature extraction and processing of all the other features (beat histogram, MFCC statistics, notes and beat-aligned chromagram) had to be implemented separately and the performance of the different MIR toolkits is shown in Figure \ref{perfex}. 

\begin{figure}[htbp]
	\centering
	\begin{tikzpicture}
	\centering
	\begin{axis}[
	    %title={Performance of various toolkits},
		x label style={at={(axis description cs:0.5,-0.05)},anchor=north},
		y label style={at={(axis description cs:-0.05,.5)},rotate=0,anchor=south},
	    xlabel={\#songs},
	    ylabel={time in s},
	    xmin=0, xmax=120,
	    ymin=0, ymax=3000,
	    xtick={0,20,40,60,80,100,120},
	    ytick={0,500,1000,1500,2000,2500,3000},
	    legend pos=north west,
	    ymajorgrids=true,
	    grid style=dashed,
	    height=8cm,
	    width=12cm,
	    grid=major,
	]
	\addplot[
		color=red,
		mark=square,
		]
		coordinates {
		(10,52)(100,385)
		};
		\addlegendentry{Essentia parallel (4 cores)}
	\addplot[
	    color=blue,
	    mark=square,
	    ]
	    coordinates {
	    (10,37)(100,442)
	    };
	    \addlegendentry{rp\_extract  (4 cores)}
	\addplot[
	    color=green,
	    mark=square,
	    ]
	    coordinates {
		(10,94)(100,1024)
	    };
	    \addlegendentry{Essentia}
	\addplot[
	    color=orange,
	    mark=square,
	    ]
	    coordinates {
	    (10,265)(100,2907)
	    };
	    \addlegendentry{librosa}	    
	\end{axis}
	\end{tikzpicture}
	\caption{Performance of various toolkits on a single computer}
	\label{perfex}
\end{figure}
\noindent In summary, the estimated time for the feature extraction of larger datasets on a single computer based on the performance measurements can be calculated and is listed below, leading to the conclusion that the features for the full dataset including the FMA dataset can only be extracted with the help of a computer cluster.
\ \\
\textbf{Estimated feature extraction times}
\begin{itemize}
	\setlength\itemsep{-0.5em}
	\item 3h24 - 1517-Artists - Essentia parallel, single-node, 4 CPU cores
	\item 3h54 - 1517-Artists - rp\_extract
	\item 9h06 - private dataset - Essentia parallel, single-node, 4 CPU cores
	\item 10h24 - private dataset - rp\_extract
	\item (125h - full dataset - Essentia parallel, single-node, 4 CPU cores)
	\item (143h - full dataset - rp\_extract)
\end{itemize}

\subsubsection{Performance on a Cluster with mpi4py}\label{mpi4py}

For the extraction of the features from the audio files of the FMA dataset on the computer cluster of the Friedrich-Schiller-University in Jena (the "ARA-cluster"), parallel python had to be replaced with mpi4py. 
Mpi4py provides Python bindings for the Message Passing Interface standard (MPI) \cite{mpi4py}. 
Every compute-process gets a rank number and is aware of the overall count of all processes. With these two values, the file list of all audio files is split, and each process only processes its respective data. The audio files were stored in a parallel cluster file system called "beegfs" \cite{beegfs}. Equally to the implementation using parallel python every process stores the results in separate output files, each of them containing batches of 25 songs.\\

\begin{pythonCode}[frame=single,label={lst:mpi4py},caption={mpi4py},captionpos=b]
comm = MPI.COMM_WORLD   # get MPI communicator object
size = comm.size        # total number of processes
rank = comm.rank        # rank of this process
status = MPI.Status()   # get MPI status object
files_per_part = 25
start = 0
last = len(filelist)
parts = (len(filelist) / files_per_part) + 1
step = (last - start) / parts + 1
for index in xrange(start + rank, last, size):
    if index < parts:        
        starti = start+index*step
        endi = min(start+(index+1)*step, last)
        parallel_python_process(index, filelist[starti:endi])
\end{pythonCode}

\noindent All audio files larger than 25MB were filtered out of the FMA dataset in advance, to avoid memory overflows, still leaving 102813 songs out of the 106733 songs to process. A total of 36 compute nodes were used. Every node had 192GB of RAM and 36 CPU cores (72 using hyper-threading (HT)). To increase the available memory per CPU, only 18 CPU cores per node were used. Overall, 648 processes were spawned. During the computation of the audio features with Essentia, one out of the 648 processes ran out of memory, so only 102793 out of the 102813 songs were processed in the end. For performance tests, this doesn't make a huge difference, but for future work, the feature extraction script should be adapted accordingly. 
The extraction of the features took 1439 seconds (fastest process) up to 1950 seconds (slowest). With better balancing and messaging between the processes, the tasks could be distributed in a way where idle tasks take parts of the file list from other tasks that are still processing. 
\noindent For the extraction of the rhythm features with the rp\_extract tool, the script of the TU Wien was adapted for usage with mpi4py as well. The same amount of processes gets spawned on the cluster (648), but each of the processes is able to make use of two CPU cores plus HT. The fastest process finished after 1657 seconds and the slowest one took 1803 seconds.

\subsubsection{Total Amount of Songs}\label{totamsong}

Due to the above-mentioned filtering of audio files larger than 25MB, the out-of-memory error of one process executing the Essentia task, and since the rhythm pattern extraction script is not able to handle some audio file formats like Ogg Vorbis, not all features from all songs could be extracted. So, in the end, the overall amount of songs where all features could be obtained is 114210.\\


