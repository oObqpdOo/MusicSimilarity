%==============================================================

\section{Big Data}\label{bdf}

After evaluating different data sources presenting various methods to extract and process different audio features, the following section describes the data analysis with Big Data processing frameworks like Apache Spark \cite{spark} and Hadoop \cite{hadoop}. Later chapter \ref{bds1} deals with the implementation of the various similarity measurements with Spark, the handling of larger amounts of data, runtime analysis and the combination of multiple similarity measurements, while chapter \ref{bds2} gives short overview over the achieved results using the Big Data framework to compare audio features. 

\subsection{Hadoop}

With the ever growing availability of huge amounts of high dimensional data the need for toolkits and efficient algorithms to handle these grew as well over the past years. They key to handle Big Data is to use parallelity.\\
Search engine providers like Google and Yahoo firstly ran into the problem of using "internet-scale" data in the early 2000s when being faced with the problem of storing and processing the ever growing amount of indexes from documents in the internet. In 2003, Google presented their whitepaper called "The Google File System" \cite{gfs}. MapReduce as a programming paradigm was introduced by google as an answer to the problem of internet scale data and dates back to 2004 when the paper "MapReduce: Simplified Data Processing on Large Clusters" was published \cite{mapreduce1}. Doug Cutting and Mike Cafarella worked on a web crawler project called Nutch during that time. Inspired by the two papers Cutting incorporated the storage and processing principles from google, leading to what we know as Hadoop today. Hadoop joined the Apache Software Foundation in 2006 \cite[p. 6]{sparkbook1}.\\ 
Hadoop is based on the idea of data locality. In contrast to the usual approach, where the data is requested from its location and transferred to a remote processing system or host, Hadoop brings the computation to the data instead. This minimizes the problem of data transfer times over network at compute time when working with very large-scale data/ Big Data. One prerequisite is that the operations on the data are independent from each other. Hadoop follows this approach called "shared-nothing". The data can be processed locally on many nodes at the same time in parallel by splitting the data in independent small subsets without the need of communicating with other nodes. Additionally Hadoop is a schemaless (schema-on-read) system which means that it is able to store and process unstructured, semi-structured (JSON, XML) or well structured data (relational database) \cite[p. 7]{sparkbook1}.\\
Hadoop is a scalable solution able to run on large computer clusters. It does not necessarily require a supercomputing environment and is able to run on clusters of lower-cost commodity hardware. The data is stored redundantly on multiple nodes with a configurable replication rate defining how many copies of each data chunk are stored on other nodes. This enables an error management where faulty operations can simply be restarted.\\
To make all this possible, Hadoop relies on its core components YARN (Yet Another Resource Negotiator) and the Hadoop Distributed File System (HDFS)\\

\textit{\underline{To Be Continued...}}

\subsubsection{MapReduce}

Figure \ref{mapred} shows the basic scheme of a MapReduce program. The core idea is to split a problem into many independent tasks.\\

\FloatBarrier
\begin{figure}[htbp]
	\centering
	\framebox{\parbox{1\textwidth}{ 
	%Image based on: https://commons.wikimedia.org/wiki/File:Mapreduce.png
	\begin{tikzpicture}[node distance = 4cm][every node/.style={thick}]
	  \colorlet{coul0}{orange!20} \colorlet{coul1}{blue!20} \colorlet{coul2}{red!20} \colorlet{coul3}{green!20}
	  \tikzstyle{edge}=[->, very thick]
	  \draw[thick, fill=violet!30] (-1, -2) rectangle node[rotate=90] {\textbf{Input data}} (0,2);
	  \foreach \i in {0,1,2,3} {
	    \node[draw, fill=coul\i, xshift=2em] (data\i) at (1.5, 1.5 - \i) {Input};
	    \node[ellipse, draw, fill=cyan!20, xshift=2em] (map\i) at (3.5, 1.5 - \i) {\textsf{Map}};
	    \draw[edge] (0,0) -- (data\i.west);
	    \draw[edge] (data\i) -- (map\i);
	  }
	  \node[draw, minimum height=2cm, fill=purple!30, xshift=7em] (resultat) at (10, 0) {\textbf{Results}};
	  \foreach \i in {0,1,2} {
	    \node[draw, fill=yellow!20, minimum width=2cm, xshift=4em] (paire\i) at (5.5, 1.5 - \i*1.5) {\begin{minipage}{1cm}Tuples \centering $\langle k,v \rangle$\end{minipage}};
	    \node[ellipse, draw, fill=cyan!20, xshift=6em] (reduce\i) at (7.5, 1.5 - \i*1.5) {\textsf{Reduce}};
	    \draw[edge] (paire\i) -- (reduce\i);
	    \draw[edge] (reduce\i.east) -- (resultat);
	  }
	  %paire
	  \draw[edge] (map0.east) -- (paire0.west); \draw[edge] (map0.east) -- (paire1.west);
	  \draw[edge] (map1.east) -- (paire0.west); \draw[edge] (map1.east) -- (paire2.west);
	  \draw[edge] (map2.east) -- (paire1.west); \draw[edge] (map2.east) -- (paire0.west);
	  \draw[edge] (map3.east) -- (paire1.west); \draw[edge] (map3.east) -- (paire2.west);
	\end{tikzpicture}
	}}
	\caption{MapReduce \cite{mapred1im}}
	\label{mapred}
\end{figure}
\FloatBarrier

\noindent In the first stage the input data is split in many chunks and distributed over the nodes of a cluster. This is usually managed by the distributed file system like the HDFS. One master nodes stores the addresses of all data chunks.\\
The data is fed into the mapper who operates on the input data and finally transforms the input into key-value tuples.\\
In an intermediate step the key-value pairs are usually grouped by their keys before being fed into the reducer. The reducer applies another method to all tuples with the same key.\\
The amount of key-value pairs at the output from the mapper divided by the number of input files is called replication rate ($r$). The largest amount of values for one key being fed into a reducer can be denoted as $q$ (reducer size). Usually there is a trade-off between a high replication rate and small $q$ (highly parallel with more network traffic) and small $r$ and larger $q$ (less network traffic but worse parallelism due to an overall smaller reducer count).

\subsection{Spark}

SQL vs NoSQL\\
\ \\
user defined function\\
\ \\
cluster configuration file\\
\ \\
streaming\\
\ \\
caching\\

\subsubsection{Cluster architecture}

\FloatBarrier
\begin{figure}[htbp]
	\centering
	\framebox{\parbox{1\textwidth}{ 
	\begin{tikzpicture}
	\tikzstyle{bigbox} = [draw=blue!60, blur shadow={shadow blur steps=5}, minimum size=2cm, thick, fill=blue!10, rounded corners, rectangle]
	\tikzstyle{box} = [draw=black!40!blue, minimum size=0.6cm, rounded corners,rectangle, fill=blue!50]
	\tikzstyle{box2} = [draw=black!60!blue, minimum size=0.6cm, rounded corners,rectangle, fill=blue!10]
	\tikzstyle{box3} = [draw=blue!80, minimum size=0.6cm, rounded corners,rectangle, fill=blue!10]
	\node[server](server 1){};
	\node[server, right of= server 1, xshift=3cm](server 2){};
	\node[server, right of= server 2, xshift=3cm](server 3){};
	\node[rack switch, above of=server 2,xshift=0.1cm,yshift=0.25cm]
	  (rack switch 1){};
   	\node[box, above of=rack switch 1, xshift=0cm, yshift=0.15cm](cm){Cluster Manager}; 
   	\node[server, above of=cm, yshift=0.15cm](servermaster){};
 	\node[box, above of=servermaster, xshift=0cm, yshift=1.25cm](sc){Spark Context}; 
  	\begin{pgfonlayer}{background}
  		\node[bigbox, yshift=-0.25cm, xshift=-0.01cm] [fit = (sc)](dr){\\ \ \\Driver};
  	\end{pgfonlayer}
	\node[box, below of=server 3, xshift=-3.25em, yshift=-0.25cm](e1){Executor 1};
	\node[box, below of=server 3, xshift=3.25em, yshift=-0.25cm](e2){Executor 2};
	\node[box, below of=server 2, xshift=0em, yshift=-0.25cm](e3){Executor 3};
	\node[box, below of=server 1, xshift=-3.25em, yshift=-0.25cm](e4){Executor 4};
	\node[box, below of=server 1, xshift=3.25em, yshift=-0.25cm](e5){Executor 5};
	\begin{pgfonlayer}{background}
		\node[bigbox, yshift=-0.35cm, xshift=-0.01cm] [fit = (e1)](mem1){\\ \ \\Memory};
	\end{pgfonlayer}
	\begin{pgfonlayer}{background}
		\node[bigbox, yshift=-0.35cm, xshift=0.01cm] [fit = (e2)](mem2){\\ \ \\Memory};
	\end{pgfonlayer}
	\begin{pgfonlayer}{background}
		\node[bigbox, yshift=-0.35cm] [fit = (e3)](mem3){\\ \ \\Memory};
	\end{pgfonlayer}
	\begin{pgfonlayer}{background}
		\node[bigbox, yshift=-0.35cm, xshift=-0.01cm] [fit = (e4)](mem4){\\ \ \\Memory};
	\end{pgfonlayer}
	\begin{pgfonlayer}{background}
		\node[bigbox, yshift=-0.35cm, xshift=0.01cm] [fit = (e5)](mem5){\\ \ \\Memory};
	\end{pgfonlayer}
	\node[box2, below of=mem1, yshift=-0.01cm](t1){Task 1};
	\node[box2, below of=mem2, yshift=-0.01cm](t2){Task 2};
	\node[box2, below of=mem3, yshift=-0.01cm](t3){Task 3};
	\node[box2, below of=mem4, yshift=-0.01cm](t4){Task 4};
	\node[box2, below of=mem5, yshift=-0.01cm](t5){Task 5};
	\node[box2, below of=t2, yshift=0.3cm](t6){Task 6};
	\node[box2, below of=t4, yshift=0.3cm](t7){Task 7};
	\node[box2, below of=t5, yshift=0.3cm](t8){Task 8};
	\draw[thick,black!60!green] (t2.south)--(t6);	
	\draw[thick,black!60!green] (t4.south)--(t7);	
	\draw[thick,black!60!green] (t5.south)--(t8);	
	\draw[thick,darkgray!10!gray] (servermaster.north)--(dr.south);
	\draw[thick,darkgray!10!gray] (servermaster.south)--(cm.north);
	\draw[thick,darkgray!10!gray] (cm.south)--(rack switch 1.north);
	\draw[thick,darkgray!10!gray] (server 1.north)--(rack switch 1);
	\draw[thick,darkgray!10!gray] (server 2.north)--(rack switch 1);
	\draw[thick,darkgray!10!gray] (server 3.north)--(rack switch 1);
	\draw[thick,darkgray!10!gray] (server 3.south)--(e1);	
	\draw[thick,darkgray!10!gray] (server 3.south)--(e2);	
	\draw[thick,darkgray!10!gray] (server 2.south)--(e3);	
	\draw[thick,darkgray!10!gray] (server 1.south)--(e4);	
	\draw[thick,darkgray!10!gray] (server 1.south)--(e5);	
	% = = = = = = = = = = = = = = = =
	% Labels
	% = = = = = = = = = = = = = = = =
	\node[box3, xshift=-6.1cm,yshift=0.3cm,left of = sc,align=left](lev1){\textbf{Main Program}};
	\node[box3, xshift=-6.25cm,yshift=0.3cm,left of = servermaster,align=left](lev2){\textbf{Master Node}};
	\node[box3, xshift=-5.85cm,yshift=0.3cm,left of = cm,align=left](lev3){\textbf{Cluster Manager}};	
	\node[box3, xshift=-6.9cm,yshift=0.3cm,left of = rack switch 1,align=left](lev4){\textbf{Switch}};
	\node[box3, xshift=-2.05cm,yshift=0.3cm,left of = server 3,align=left](lev5){\textbf{Executor Nodes}};
	\node[box3, xshift=-1.325cm,yshift=0.3cm,left of = e1,align=left](lev6){\textbf{Executors}};
	\end{tikzpicture}
	}}
	\caption{Spark Cluster}
	\label{dataloc}
\end{figure}
\FloatBarrier


\subsubsection{Spark and RDDs}

\subsubsection{Spark DataFrame}

\subsubsection{lazy evaluation}

\subsubsection{min and max value aggregation}

\subsubsection{Hadoop vs Spark Memory Management}

\subsubsection{Spark application UI}

\subsection{Lambda architecture}

Other Big Data Toolkits: SMACK; lambda; spark\\

Batch-Layer\\
\ \\
Speed-Layer\\
\ \\
Serving-Layer\\

\subsection{Combining different measurements}

\noindent\textit{\textbf{weighted arithmetic mean of different distance measurements - there has to be some kind of scaling\\}}

\begin{equation} \label{eq:distance}
dist = \frac{\sum_{m = 0}^{M - 1}{w_m \cdot d_m}}{\sum_{m = 0}^{M - 1}{w_m}}
\end{equation}

\noindent\textit{\textbf{statistic prescaling to have mean = 0.5 and variance 0.5?\\}}

\textit{\textbf{SPARK NOT ABLE TO PROCESS AUDIO FILES -> SEPARATE TASK}}

\subsection{Data locality and parallelization}

Given the short introduction to Big Data frameworks the decision to use Spark for the computation of the similarities is justifiable. The computation of a one-to-many similarity follows the shared nothing approach of Spark. All the features are independent from each other. Only the scaling of the result requires an aggregation of maximum and minimum values and to return the top results some kind of sorting has to be performed. But apart from these operations, all the features can be distributed and the similarity to one broadcasted song can be calculated independently, following the data locality approach. This offers a fully scalable solution for very large datasets.

\noindent\textit{\textbf{high data locality, full parallelizable, very low replication rate\\}}
\noindent\textit{\textbf{insert data locality scheme here\\}}
